{
  "name": "QLoRA",
  "description": "Efficient fine-tuning approach that reduces memory usage by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low-Rank Adapters (LoRA), enabling fine-tuning of large models on limited hardware without performance loss.",
  "method_type": [
    "Quantization"
  ],
  "gpus_required": "Low",
  "training_speed": "Fast",
  "source": "https://arxiv.org/abs/2305.14314",
  "code_url": "https://github.com/artidoro/qlora",
  "open_source": false,
  "intended_use": [
    "Resource-Constrained Fine-Tuning"
  ]
}